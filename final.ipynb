{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from xgboost import XGBRegressor\n",
    "from pyswarm import pso\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preprocessing\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Scale demand\n",
    "    scaler = MinMaxScaler()\n",
    "    data['Peak Demand'] = scaler.fit_transform(data[['Peak Demand']])\n",
    "\n",
    "    return data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Engineering (Adjusted)\n",
    "def create_features(data):\n",
    "    # data['day_of_week'] = data.index.dayofweek   #Removing as it can take only discrete value\n",
    "    # data['month'] = data.index.month #Removing as it can take only discrete value\n",
    "    # data['year'] = data.index.year  #Removing as it can take only discrete value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare LSTM Data (Adjusted for Continuous Data)\n",
    "def prepare_lstm_data(data, time_steps=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:(i + time_steps)].values)\n",
    "        y.append(data['Peak Demand'].iloc[i + time_steps])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hybrid Model Building Blocks\n",
    "def build_lstm_attention(input_shape):\n",
    "    lstm_input = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(64, return_sequences=True)(lstm_input)\n",
    "    attention_out = Attention()([lstm_out, lstm_out])\n",
    "    lstm_output = LSTM(32)(attention_out)\n",
    "    return lstm_input, lstm_output\n",
    "\n",
    "def build_xgboost(n_estimators=100):\n",
    "    xgb = XGBRegressor(objective='reg:squarederror', n_estimators=n_estimators)\n",
    "    return xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Hybrid Model Creation\n",
    "def create_hybrid_model(lstm_input_shape, num_xgb_features, lstm_units=32, learning_rate=0.001, xgboost_estimators=100):\n",
    "    # LSTM branch\n",
    "    lstm_input, lstm_output = build_lstm_attention(lstm_input_shape)\n",
    "\n",
    "    # XGBoost branch\n",
    "    xgb_input = Input(shape=(num_xgb_features,))\n",
    "    xgb_output = Dense(16)(xgb_input)\n",
    "\n",
    "    # Combine\n",
    "    combined = concatenate([lstm_output, xgb_output])\n",
    "    dense1 = Dense(32, activation='relu')(combined)\n",
    "    output = Dense(1, activation='linear')(dense1)\n",
    "\n",
    "    model = Model(inputs=[lstm_input, xgb_input], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PSO Optimization (Adjusted Objective Function)\n",
    "def objective_function(params, X_lstm_train, X_xgb_train, y_train, lstm_input_shape, num_xgb_features):\n",
    "    # Example: params = [lstm_units, learning_rate, xgboost_estimators]\n",
    "    lstm_units = int(params[0])  # Ensure integer\n",
    "    learning_rate = params[1]\n",
    "    xgboost_estimators = int(params[2])\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_hybrid_model(lstm_input_shape, num_xgb_features, lstm_units, learning_rate, xgboost_estimators)\n",
    "    model.fit([X_lstm_train, X_xgb_train], y_train, epochs=2, verbose=0)\n",
    "    loss = model.evaluate([X_lstm_train, X_xgb_train], y_train, verbose=0)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping search: maximum iterations reached --> 5\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 917.0349\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 6.3906\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.5525\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.4189\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.2839\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1991\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1270\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0823\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0465\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0329\n"
     ]
    }
   ],
   "source": [
    "# 7. Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess\n",
    "    file_path = 'final_dataset.csv'\n",
    "    data, demand_scaler = load_and_preprocess_data(file_path)\n",
    "\n",
    "    # Feature creation\n",
    "    data = create_features(data)\n",
    "\n",
    "    # Prepare LSTM data\n",
    "    time_steps = 30\n",
    "    X_lstm, y = prepare_lstm_data(data[['Peak Demand', 'Temperature', 'Relative Humidity']], time_steps)\n",
    "\n",
    "    # Prepare XGBoost data (Use last values from LSTM sequence - adjust features as needed)\n",
    "    X_xgb = data[['Temperature', 'Relative Humidity']][time_steps:].values\n",
    "    \n",
    "    # Split data\n",
    "    X_lstm_train, X_lstm_test, X_xgb_train, X_xgb_test, y_train, y_test = train_test_split(\n",
    "        X_lstm, X_xgb, y, test_size=0.2, shuffle=False\n",
    "    )\n",
    "\n",
    "    # PSO Optimization\n",
    "    lb = [32, 0.001, 50]  # Lower bounds [lstm_units, learning_rate, xgboost_estimators]\n",
    "    ub = [128, 0.1, 200] # Upper bounds [lstm_units, learning_rate, xgboost_estimators]\n",
    "\n",
    "    lstm_input_shape = (X_lstm_train.shape[1], X_lstm_train.shape[2])\n",
    "    num_xgb_features = X_xgb_train.shape[1]\n",
    "    \n",
    "\n",
    "    # Ensure correct arguments are passed\n",
    "    def pso_objective(params):\n",
    "        return objective_function(params, X_lstm_train, X_xgb_train, y_train, lstm_input_shape, num_xgb_features)\n",
    "\n",
    "    best_params, _ = pso(pso_objective, lb, ub, swarmsize=10, maxiter=5)\n",
    "\n",
    "    # Create and train the final model with optimized parameters\n",
    "    lstm_units = int(best_params[0])\n",
    "    learning_rate = best_params[1]\n",
    "    xgboost_estimators = int(best_params[2])\n",
    "\n",
    "    final_model = create_hybrid_model(lstm_input_shape, num_xgb_features, lstm_units, learning_rate, xgboost_estimators)\n",
    "    final_model.fit([X_lstm_train, X_xgb_train], y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted Peak Demand for 2024-04-20 : 211.8589 MW\n",
      "Error: time data '2024-13-12' does not match format '%Y-%m-%d'\n"
     ]
    }
   ],
   "source": [
    "# -- Make prediction for a specific date --\n",
    "while True:\n",
    "    input_date_str = input(\"Enter date to predict (YYYY-MM-DD, or type 'exit'): \")\n",
    "    if input_date_str.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        input_date = datetime.strptime(input_date_str, '%Y-%m-%d').date()  # Parse to date only\n",
    "\n",
    "        # Find data for the entered date or use available date to estimate\n",
    "        if input_date_str in data.index:\n",
    "            input_data = data[['Temperature', 'Relative Humidity']].loc[input_date_str] #Data is collected from already made data\n",
    "        else:\n",
    "            raise ValueError(\"Provided date does not exist in the list of dates in dataset. Choose new date or type exit.\")\n",
    "\n",
    "        # Get the last 'time_steps' days of data before this date\n",
    "        #print(data.index)\n",
    "\n",
    "        #Prepare dataframes for input and check to see whether they are empty\n",
    "        last_data = data[['Peak Demand', 'Temperature', 'Relative Humidity']].loc[data.index < input_date_str].iloc[-time_steps:]\n",
    "\n",
    "        if last_data.empty:\n",
    "            raise ValueError(\"Not enough historical data for the given date.\")\n",
    "\n",
    "        # Create the input for LSTM\n",
    "        X_lstm_next_day = np.array([last_data.values])\n",
    "\n",
    "        # Create the input for XGBoost:\n",
    "        X_xgb_next_day = np.array([input_data.values])\n",
    "\n",
    "        # Make the prediction:\n",
    "        predicted_peak_demand = final_model.predict([X_lstm_next_day, X_xgb_next_day.reshape(1,-1)])\n",
    "\n",
    "        # Inverse Transform for scaling\n",
    "        predicted_peak_demand = demand_scaler.inverse_transform(predicted_peak_demand)\n",
    "\n",
    "        # Print results\n",
    "        print(\"Predicted Peak Demand for\", input_date_str, \":\", predicted_peak_demand[0][0], \"MW\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Peak Demand for Tomorrow: 244.2282 MW\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Make prediction for the next day\n",
    "# Prepare input data for the next day forecast:\n",
    "\n",
    "# Get the last 'time_steps' days of data\n",
    "last_data = data[['Peak Demand', 'Temperature', 'Relative Humidity']].iloc[-time_steps:]\n",
    "\n",
    "# Create the input for LSTM\n",
    "X_lstm_next_day = np.array([last_data.values])\n",
    "\n",
    "# Create the input for XGBoost (using last values from LSTM sequence)\n",
    "X_xgb_next_day = data[['Temperature', 'Relative Humidity']].iloc[[-1]].values\n",
    "# Make the prediction:\n",
    "predicted_peak_demand = final_model.predict([X_lstm_next_day, X_xgb_next_day])\n",
    "\n",
    "# Inverse Transform for scaling\n",
    "predicted_peak_demand = demand_scaler.inverse_transform(predicted_peak_demand)\n",
    "\n",
    "# print results\n",
    "print(\"Predicted Peak Demand for Tomorrow:\", predicted_peak_demand[0][0], \"MW\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
